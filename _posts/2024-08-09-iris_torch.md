# ì‹¬í™” ê³¼ì œ Without `torch.nn`
---

## 1) ê³¼ì œ ê°œìš”
- PyTorchì˜ `torch.nn`ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (Multi-Layer Perceptron, MLP)ì„ êµ¬í˜„í•˜ê³  ì´í›„ `torch.nn`ì„ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•œ MLPì™€ ë¹„êµí•©ë‹ˆë‹¤.
- 3ê°€ì§€ ì´ìƒ ì¹´í…Œê³ ë¦¬ë¥¼ ë¶„ë¥˜í•˜ëŠ” Multi-class classificationì„ êµ¬í˜„í•©ë‹ˆë‹¤.

## 2) ê³¼ì œ ì§„í–‰ ëª©ì  ë° ë°°ê²½
- `torch.nn`ì—ëŠ” ëª¨ë¥´ê³  ì‚¬ìš©í•˜ëŠ” ê¸°ëŠ¥ë“¤ì´ ë§ì•„ ì˜¤ìš©í•˜ëŠ” ê²½ìš°ë¥¼ ì¤„ì´ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.

## 3) ê³¼ì œ ìˆ˜í–‰ìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆëŠ” ì—­ëŸ‰
- MLPë¥¼ êµ¬í˜„í•˜ëŠ” ê³¼ì •ì„ í†µí•´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ êµ¬ì¡°ì™€ í•™ìŠµ ë°©ë²•ì„ ì´í•´í•©ë‹ˆë‹¤.
- `torch.nn`ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  MLPë¥¼ êµ¬í˜„í•˜ëŠ” ê³¼ì •ì„ í†µí•´ PyTorchì˜ ê¸°ëŠ¥ì„ ì´í•´í•©ë‹ˆë‹¤.


## 4) ê³¼ì œ í•µì‹¬ ë‚´ìš©
1. Iris ë°ì´í„° ì…‹ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. [ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°](#scrollTo=JMviOiQCTGzR)
2. `torch.nn` ì—†ì´ MLP ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤. [ëª¨ë¸ êµ¬í˜„(Without `torch.nn`)](#scrollTo=ZUZiZOJVTGYb)
3. ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤. [ëª¨ë¸ í•™ìŠµ(Without `torch.nn`)](#scrollTo=ZDJxT_xGTGYd)
4. `torch.nn`ì„ ì‚¬ìš©í•˜ì—¬ MLP ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤. [ëª¨ë¸ êµ¬í˜„(With `torch.nn`)](#scrollTo=pYqi8L12TGYe)
5. ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤. [ëª¨ë¸ í•™ìŠµ](#scrollTo=ZDJxT_xGTGYd)
6. ë‘ ëª¨ë¸ì„ ë¹„êµí•©ë‹ˆë‹¤. [ë¹„êµ](#scrollTo=JRFd2onlTGYf)


## 5) ë°ì´í„° ì…‹ ê°œìš” ë° ì €ì‘ê¶Œ ì •ë³´
>Iris ë°ì´í„°ì…‹ì€ ê½ƒìê³¼ ê½ƒë°›ì¹¨ì˜ ê¸¸ì´ì™€ ë„ˆë¹„ë¥¼ ì´ìš©í•˜ì—¬ iris ê½ƒì˜ í’ˆì¢…ì„ ë¶„ë¥˜í•˜ëŠ” ë°ì´í„°ì…‹ì…ë‹ˆë‹¤. ì´ 3ê°œì˜ class(*celss*, *versicolor*, *virginica*)ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ê° classëŠ” iris ê½ƒì˜ í’ˆì¢…ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. iris ë°ì´í„°ì…‹ì€ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

- iris data: iris ê½ƒì˜ ê½ƒìê³¼ ê½ƒë°›ì¹¨ì˜ ê¸¸ì´ì™€ ë„ˆë¹„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” featureì…ë‹ˆë‹¤.
- iris target: iris ê½ƒì˜ í’ˆì¢…ì„ ë‚˜íƒ€ë‚´ëŠ” labelì…ë‹ˆë‹¤.
- UCI Machine Learning Repository: "Iris" Data Set - [Iris Data Set Description](https://archive.ics.uci.edu/ml/datasets/iris)

**License**
- This dataset is licensed under a [Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/legalcode) (CC BY 4.0) license.

## 6) Required Packages
```python
sklearn >= 1.2.2
torch >= 2.2.1
matplotlib >= 3.7.1
numpy >= 1.25.2
pandas >= 2.0.3
```


---
## ê³¼ì œ ì§„í–‰ ë°©ë²•

`[CODE START]`ì™€ `[CODE END]` ì‚¬ì´ì˜ ì½”ë“œë“¤ì„ ì™„ì„±í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

- ğŸš¨ì£¼ì˜
    - ì½”ë“œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.

---

### 1. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°

**â“ë¬¸ì œ**
> Iris ë°ì´í„° ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê³  ì´ë¥¼ PyTorchì˜ Dataset í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ ì •ì˜ëœ í´ë˜ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
1. `__init__` ë©”ì†Œë“œì—ì„œ `self.X`ì™€ `self.y`ë¥¼ ê°ê° iris ë°ì´í„°ì™€ iris targetìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
2. `__len__` ë©”ì†Œë“œì—ì„œ ë°ì´í„°ì…‹ì˜ ê¸¸ì´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
3. `__getitem__` ë©”ì†Œë“œì—ì„œ indexì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ì™€ ë ˆì´ë¸”ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

**Furthermore info**

ì½”ë“œ ì¤‘ ê³¼ì œì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ë¶€ë¶„ì˜ ì½”ë“œì— ëŒ€í•œ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
- `sklearn.datasets.load_iris()`
    - iris ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    - [ğŸ“šDocumentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)
- `sklearn.model_selection.train_test_split()`
    - ë°ì´í„°ì…‹ì„ trainê³¼ testë¡œ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    - [ğŸ“šDocumentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)


```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset
import torch


class IrisDataset(Dataset):
    def __init__(self, mode="train", random_state=0):
        """
        mode: ë°ì´í„°ì…‹ì˜ ìš©ë„ë¥¼ ì§€ì •í•˜ëŠ” ë¬¸ìì—´ì…ë‹ˆë‹¤. "train"ì´ë©´ í•™ìŠµ ë°ì´í„°ì…‹ì„, ê·¸ ì™¸ì˜ ê°’ì´ë©´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë¡œë”©í•©ë‹ˆë‹¤.
        random_state: ë°ì´í„° ë¶„í•  ì‹œ ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•´ ë‚œìˆ˜ ìƒì„±ê¸°ì— ì‚¬ìš©ë˜ëŠ” ì‹œë“œ ê°’ì…ë‹ˆë‹¤.
        """
        iris = load_iris()
        train_X, test_X, train_y, test_y = train_test_split(
            iris.data,
            iris.target,
            stratify=iris.target,
            test_size=0.2,
            random_state=random_state,
        )
        if mode == "train":
            self.X = torch.FloatTensor(train_X)
            self.y = torch.LongTensor(train_y)

        else:
            self.X = torch.FloatTensor(test_X)
            self.y = torch.LongTensor(test_y)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        X = self.X[idx]
        y = self.y[idx]

        return X, y
```

#### 1.1 ë°ì´í„° ì…‹ í™•ì¸

ìœ„ì—ì„œ ì™„ì„±í•œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„° ì…‹ì„ ì„ ì–¸í•˜ê³  í™•ì¸í•©ë‹ˆë‹¤.

**â“ë¬¸ì œ**
1. `IrisDataset` í´ë˜ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
2. `torch.utils.data.Dataloader`ë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„° ë¡œë”ë¥¼ ì„ ì–¸í•©ë‹ˆë‹¤.


```python
batch_size = 16

train_dataset = IrisDataset(mode="train")
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

test_dataset = IrisDataset(mode="test")
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
```


```python
# ì…ˆí”Œ ë°ì´í„° í™•ì¸
X, y = next(iter(train_loader))

print("X feature ë°ì´í„° í™•ì¸")
print(X)
print(X.shape)
print("y target ë°ì´í„° í™•ì¸")
print(y)
print(y.shape)

# check shape
assert X.shape == (16, 4)
assert y.shape == (16,)

# check data type
assert X.dtype == torch.float32
assert y.dtype == torch.int64
print("âœ…í…ŒìŠ¤íŠ¸ í†µê³¼!ğŸ¥³")
```

    X feature ë°ì´í„° í™•ì¸
    tensor([[6.4000, 3.1000, 5.5000, 1.8000],
            [6.2000, 3.4000, 5.4000, 2.3000],
            [5.3000, 3.7000, 1.5000, 0.2000],
            [4.4000, 3.2000, 1.3000, 0.2000],
            [6.1000, 3.0000, 4.9000, 1.8000],
            [4.8000, 3.4000, 1.6000, 0.2000],
            [5.7000, 2.6000, 3.5000, 1.0000],
            [4.4000, 2.9000, 1.4000, 0.2000],
            [6.7000, 3.0000, 5.0000, 1.7000],
            [6.2000, 2.9000, 4.3000, 1.3000],
            [7.2000, 3.2000, 6.0000, 1.8000],
            [5.8000, 4.0000, 1.2000, 0.2000],
            [5.8000, 2.7000, 5.1000, 1.9000],
            [6.6000, 3.0000, 4.4000, 1.4000],
            [5.4000, 3.9000, 1.7000, 0.4000],
            [6.3000, 3.3000, 4.7000, 1.6000]])
    torch.Size([16, 4])
    y target ë°ì´í„° í™•ì¸
    tensor([2, 2, 0, 0, 2, 0, 1, 0, 1, 1, 2, 0, 2, 1, 0, 1])
    torch.Size([16])
    âœ…í…ŒìŠ¤íŠ¸ í†µê³¼!ğŸ¥³
    

### 2. ëª¨ë¸ êµ¬í˜„(Without `torch.nn`)

#### 2.1.1 Define Linear layer

**â“ë¬¸ì œ**
1. `__init__` ë©”ì†Œë“œì—ì„œ `self.weight`ì™€ `self.bias`ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
2. `__call__` ë©”ì†Œë“œì—ì„œ ì…ë ¥ê°’ `x`ë¥¼ ë°›ì•„ ì„ í˜• ë³€í™˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
3. `set_device` ë©”ì†Œë“œì—ì„œ ì—°ì‚°ì„ ìˆ˜í–‰í•  `device`ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
4. `parameters` ë©”ì†Œë“œì—ì„œ `weight`ì™€ `bias`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.



```python
import math


# ì„ í˜• ë ˆì´ì–´ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ ë ˆì´ì–´ëŠ” ì…ë ¥ íŠ¹ì„±ê³¼ ì¶œë ¥ íŠ¹ì„±ì˜ ìˆ˜ë¥¼ ë°›ì•„ì„œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
class WithoutNNLinear:
    def __init__(self, in_features, out_features):
        self.weight = torch.randn(
            out_features, in_features
        )  # ê°€ì¤‘ì¹˜ëŠ” ëœë¤í•˜ê²Œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        self.bias = torch.randn(out_features)  # í¸í–¥ë„ ëœë¤í•˜ê²Œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

    # ì´ ë©”ì†Œë“œëŠ” ì…ë ¥ xë¥¼ ë°›ì•„ì„œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì´ìš©í•˜ì—¬ ì¶œë ¥ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    def __call__(self, x):
        return (
            x @ self.weight.t() + self.bias
        )  # xì™€ ê°€ì¤‘ì¹˜ì˜ í–‰ë ¬ ê³±ì…ˆì„ ìˆ˜í–‰í•˜ê³  í¸í–¥ì„ ë”í•©ë‹ˆë‹¤.

    # ì´ ë©”ì†Œë“œëŠ” ëª¨ë¸ì„ íŠ¹ì • ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.
    def set_device(self, device):
        self.weight = self.weight.to(device)
        self.bias = self.bias.to(device)

    # ì´ ë©”ì†Œë“œëŠ” ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    def parameters(self) -> list:
        """
        ë°˜í™˜ê°’:
            list: ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ í…ì„œë¥¼ í¬í•¨í•˜ëŠ” ë¦¬ìŠ¤íŠ¸.
        """
        return [self.weight, self.bias]
```

#### 2.1.2 Linear layer debug

>ì˜ë„í•œ ëŒ€ë¡œ `WithoutNNLinear` í´ë˜ìŠ¤ê°€ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.


```python
without_nn_linear = WithoutNNLinear(1, 3)

x = torch.FloatTensor([[1.0]])
print("ë³€ê²½ ì „ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ í™•ì¸")
print(without_nn_linear.weight)
print(without_nn_linear.bias)

# ì„ í˜• ë ˆì´ì–´ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤.
without_nn_linear.weight = torch.FloatTensor([[1.0], [2.0], [3.0]])
without_nn_linear.bias = torch.FloatTensor([1.0, 1.0, 1.0])
print("ë³€ê²½ëœ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ í™•ì¸")
print(without_nn_linear.weight)
print(without_nn_linear.bias)

out = without_nn_linear(x)
print("ë³€ê²½ëœ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ í†µê³¼í•œ ì¶œë ¥ í™•ì¸")
print(out)

assert out.shape == (1, 3)
assert torch.allclose(out, torch.FloatTensor([[2.0, 3.0, 4.0]]))
print("âœ…í…ŒìŠ¤íŠ¸ í†µê³¼!ğŸ¥³")
```

    ë³€ê²½ ì „ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ í™•ì¸
    tensor([[-1.2691],
            [ 0.2521],
            [-0.3986]])
    tensor([-1.2099, -1.0097, -0.0662])
    ë³€ê²½ëœ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ í™•ì¸
    tensor([[1.],
            [2.],
            [3.]])
    tensor([1., 1., 1.])
    ë³€ê²½ëœ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ í†µê³¼í•œ ì¶œë ¥ í™•ì¸
    tensor([[2., 3., 4.]])
    âœ…í…ŒìŠ¤íŠ¸ í†µê³¼!ğŸ¥³
    

#### 2.1.3 Linear layerì˜ ìˆ˜í•™ì  ê³„ì‚°

ì„ í˜• ê³„ì¸µì€ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì„ í˜• ê³„ì¸µì˜ íŒŒë¼ë¯¸í„° ê°’ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- ê°€ì¤‘ì¹˜ í–‰ë ¬:
$$
W = \begin{bmatrix}
1.0 \\
2.0 \\
3.0 \\
\end{bmatrix}
$$

- í¸í–¥ ë²¡í„°:
$$b = \begin{bmatrix} 1.0 & 1.0 & 1.0 \end{bmatrix}$$


- ì…ë ¥ ë°ì´í„°:
$$
x = \begin{bmatrix}
1.0
\end{bmatrix}
$$



ì„ í˜• ê³„ì¸µì˜ ì—°ì‚°ì€ ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$y = xW^T + b$$

1. ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ì „ì¹˜í•©ë‹ˆë‹¤.
$$W^T = \begin{bmatrix} 1.0 & 2.0 & 3.0 \end{bmatrix}
$$

2. ì…ë ¥ ë°ì´í„°ì™€ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ í–‰ë ¬ê³±í•©ë‹ˆë‹¤.
$$
xW^T = \begin{bmatrix}
1.0
\end{bmatrix} \begin{bmatrix}
1.0 & 2.0 & 3.0
\end{bmatrix}
$$

$$
xW^T = \begin{bmatrix}
1.0 * 1.0 & 1.0 * 2.0 & 1.0 * 3.0
\end{bmatrix}
$$

$$
xW^T = \begin{bmatrix}
1.0 & 2.0 & 3.0
\end{bmatrix}
$$

3. í¸í–¥ ë²¡í„°ë¥¼ ë”í•©ë‹ˆë‹¤.
$$
xW^T + b = \begin{bmatrix}
1.0 & 2.0 & 3.0
\end{bmatrix} + \begin{bmatrix}
1.0 & 1.0 & 1.0
\end{bmatrix}
$$

4. ìµœì¢… ê²°ê³¼
$$
y = \begin{bmatrix}
2.0 & 3.0 & 4.0
\end{bmatrix}
$$


#### 2.2.1 Define ReLU layer

ReLU(Rectified Linear Unit)ëŠ” ì‹ ê²½ë§, íŠ¹íˆ ë”¥ëŸ¬ë‹ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ReLU í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤:

$$
\text{ReLU}(x) = \max(0, x)
$$

ì¦‰, ì…ë ¥ì´ ì–‘ìˆ˜ì´ë©´ ì…ë ¥ ê°’ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0ì„ ì¶œë ¥í•©ë‹ˆë‹¤. ì´ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ìš”ì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

$$
\text{ReLU}(x) =
\begin{cases}
x & \text{if } x > 0 \\
0 & \text{if } x \leq 0
\end{cases}
$$

**â“ë¬¸ì œ**
1. `__call__` ë©”ì†Œë“œì—ì„œ ì…ë ¥ê°’ `x`ë¥¼ ë°›ì•„ ReLU ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.


```python
# ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì…ë ¥ xë¥¼ ë°›ì•„ì„œ 0ë³´ë‹¤ í° ê°’ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
class WithoutNNReLU:
    def __init__(self):
        pass

    def __call__(self, x):
        return x.clamp(min=0)  # xì˜ ëª¨ë“  ì›ì†Œì— ëŒ€í•´ 0ë³´ë‹¤ ì‘ì€ ê°’ì€ 0ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
```

#### 2.2.2 ReLU layer debug

ì˜ë„í•œ ëŒ€ë¡œ `WithoutNNReLU` í´ë˜ìŠ¤ê°€ ì‘ë™í•˜ëŠ”ì§€ ê·¸ë˜í”„ì™€ ê°’ì„ ë¹„êµí•˜ì—¬ í™•ì¸í•©ë‹ˆë‹¤.


```python
without_relu = WithoutNNReLU()
x = torch.linspace(-0.5, 0.5, 11)
print("ReLU ì ìš© ì „")
print(x)
out = without_relu(x)
print("ReLU ì ìš© í›„")
print(out)

import matplotlib.pyplot as plt

plt.plot(x, x)
plt.plot(x, out)
plt.show()

assert torch.allclose(
    out, torch.FloatTensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5])
)
print("âœ…í…ŒìŠ¤íŠ¸ í†µê³¼!ğŸ¥³")
```

    ReLU ì ìš© ì „
    tensor([-0.5000, -0.4000, -0.3000, -0.2000, -0.1000,  0.0000,  0.1000,  0.2000,
             0.3000,  0.4000,  0.5000])
    ReLU ì ìš© í›„
    tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.2000, 0.3000,
            0.4000, 0.5000])
    


    
![png](output_17_1.png)
    


    âœ…í…ŒìŠ¤íŠ¸ í†µê³¼!ğŸ¥³
    

#### 2.3.1 Define MLP(Multi-Layer Perceptron)

ìœ„ì—ì„œ ë§Œë“  Linear layerì™€ ReLU layerë¥¼ ì´ìš©í•˜ì—¬ MLPë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

**â“ë¬¸ì œ**
1. `__init__` ë©”ì†Œë“œì—ì„œ `self.linear1`, `self.relu1`, `self.linear2`, `self.relu2`ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. (ìœ„ì—ì„œ ì •ì˜í•œ `WithoutNNLinear`ê³¼ `WithoutNNReLU` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.)
2. `__call__` ë©”ì†Œë“œì—ì„œ ì…ë ¥ê°’ `x`ë¥¼ ë°›ì•„ MLP ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
3. `set_device` ë©”ì†Œë“œì—ì„œ ì—°ì‚°ì„ ìˆ˜í–‰í•  `device`ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
4. `parameters` ë©”ì†Œë“œì—ì„œ `weight`ì™€ `bias`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.



```python
# MLPë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì„ í˜• ë ˆì´ì–´ì™€ ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ êµ¬ì„±ë©ë‹ˆë‹¤.
class WithoutNNMLP:
    def __init__(self, in_features, hidden_features, out_features):
        self.linear1 = WithoutNNLinear(
            in_features, hidden_features
        )  # ì²« ë²ˆì§¸ ì„ í˜• ë ˆì´ì–´
        self.relu = WithoutNNReLU()  # ReLU í™œì„±í™” í•¨ìˆ˜
        self.linear2 = WithoutNNLinear(
            hidden_features, out_features
        )  # ë‘ ë²ˆì§¸ ì„ í˜• ë ˆì´ì–´

    # ì´ ë©”ì†Œë“œëŠ” ì…ë ¥ xë¥¼ ë°›ì•„ì„œ ëª¨ë¸ì„ í†µê³¼ì‹œí‚¨ í›„ ì¶œë ¥ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    def __call__(self, x):
        x = self.linear1(x)
        x = self.relu(x)
        x = self.linear2(x)
        return x

    # ì´ ë©”ì†Œë“œëŠ” ëª¨ë¸ì„ íŠ¹ì • ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.
    def set_device(self, device):
        self.linear1.set_divce(device)
        self.linear2.set_divce(device)

    # ì´ ë©”ì†Œë“œëŠ” ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    def parameters(self) -> list:
        """
        ë°˜í™˜ê°’:
            list: ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ í…ì„œë¥¼ í¬í•¨í•˜ëŠ” ë¦¬ìŠ¤íŠ¸.
        """
        return self.linear1.parameters() + self.linear2.parameters()
```

#### 2.4.1 Define CrossEntropyLoss

**â“ë¬¸ì œ**
- ì•„ë˜ ê³µì‹ì„ ì°¸ê³ í•˜ì—¬ Cross Entropy Lossë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

$$
\hat{y}_{i,c} = \text{softmax}(z_{i,c}) = \frac{e^{z_{i,c}}}{\sum_{j=1}^{C} e^{z_{i,j}}}
$$

- $z_i$ëŠ” í´ë˜ìŠ¤ $i$ì— ëŒ€í•œ ì…ë ¥ ê°’ $logit$
- $\sum_{j=1}^{c} e^{z_j}$ëŠ” ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•œ ì…ë ¥ ê°’ì˜ ì§€ìˆ˜ì˜ í•©

$$
L = - \frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log(\hat{y}_{i,c})
$$

- $N$ì€ ìƒ˜í”Œì˜ ìˆ˜
- $C$ëŠ” í´ë˜ìŠ¤ì˜ ìˆ˜

**ğŸ’¡Hints**
- âš ï¸10ê°•ì—ì„œ í•™ìŠµí•œ Binary CrossEntropy loss ê³µì‹ì´ ë‹¤ë¥´ë‹ˆ ìœ ì˜í•˜ì„¸ìš”.
- `torch.log_softmax`: softmax í•¨ìˆ˜ë¥¼ ì ìš©í•œ í›„ logë¥¼ ì·¨í•©ë‹ˆë‹¤. [link](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html)
- `torch.softmax`: softmax í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤. [link](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)


```python
# Cross Entropy ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì¶œë ¥ê³¼ íƒ€ê²Ÿì„ ë°›ì•„ì„œ ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
class WithoutNNCrossEntropyLoss:
    def __init__(self, reduce="mean"):
        self.reduce = reduce

    # ì´ ë©”ì†Œë“œëŠ” ì¶œë ¥ê³¼ íƒ€ê²Ÿì„ ë°›ì•„ì„œ ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    def __call__(self, output, target):
        return -torch.log_softmax(output, dim=1)[range(target.size(0)), target].mean()
```

#### 2.4.2 CrossEntropyLoss debug
>ì˜ë„í•œ ëŒ€ë¡œ `WithoutNNCrossEntropyLoss` í´ë˜ìŠ¤ê°€ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.


```python
ce_loss = WithoutNNCrossEntropyLoss()
output = torch.FloatTensor([[0.1, 0.2, 0.7], [0.2, 0.3, 0.5]])  # ëª¨ë¸ì˜ ì¶œë ¥
target = torch.LongTensor([2, 1])  # íƒ€ê²Ÿ
loss = ce_loss(output, target)  # ì†ì‹¤ ê³„ì‚°
print("Cross Entropy ì†ì‹¤ í™•ì¸")
print(loss)

assert torch.allclose(loss, torch.FloatTensor([0.9539]), atol=1e-4)
print("âœ…í…ŒìŠ¤íŠ¸ í†µê³¼!ğŸ¥³")
```

    Cross Entropy ì†ì‹¤ í™•ì¸
    tensor(0.9539)
    âœ…í…ŒìŠ¤íŠ¸ í†µê³¼!ğŸ¥³
    


```python
import torch.nn as nn
nn_ce_loss = nn.CrossEntropyLoss()
nn_loss = nn_ce_loss(output, target)
print("Cross Entropy ì†ì‹¤ í™•ì¸")
print(nn_loss)
```

    Cross Entropy ì†ì‹¤ í™•ì¸
    tensor(0.9539)
    

### 3. ëª¨ë¸ í•™ìŠµ(Without `torch.nn`)

**â“ë¬¸ì œ**
1. í•™ìŠµ, í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…‹ê³¼ ë°ì´í„° ë¡œë”ë¥¼ ì„ ì–¸í•©ë‹ˆë‹¤.
2. `train` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í•™ìŠµ lossë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
3. `test` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ í‰ê°€í•˜ê³  í‰ê°€ lossì™€ accuracyë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.


```python
# í•™ìŠµ ë°ì´í„°ì…‹ê³¼ ë°ì´í„° ë¡œë”ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
batch_size = 16
train_dataset = IrisDataset(mode="train")
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ê³¼ ë°ì´í„° ë¡œë”ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
test_dataset = IrisDataset(mode="test")
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)


# í•™ìŠµ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ë°ì´í„° ë¡œë”ë¥¼ ë°›ì•„ì„œ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
def train(model, criterion, optimizer, train_loader) -> float:

    # ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ modelì˜ ë¯¸ë¶„ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •í•©ë‹ˆë‹¤.
    for param in model.parameters():
        param.requires_grad = True

    running_loss = 0
    for X, y in train_loader:
        optimizer.zero_grad()  # ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        output = model(X)  # ëª¨ë¸ì˜ ì¶œë ¥ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        loss = criterion(output, y)  # lossì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        loss.backward()  # ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        optimizer.step()  # íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        running_loss += loss.item()  # lossì„ ëˆ„ì í•©ë‹ˆë‹¤.
    return running_loss / len(train_loader)  # í‰ê·  lossì„ ë°˜í™˜í•©ë‹ˆë‹¤.


def test(model, test_loader, criterion) -> tuple[float, float]:

    running_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():# ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ì§€ ì•Šê²Œ ì„¤ì •í•©ë‹ˆë‹¤.
        for X, y in test_loader:
            output = model(X)  # ëª¨ë¸ì˜ ì¶œë ¥ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
            loss = criterion(output, y)  # lossì„ ê³„ì‚°í•©ë‹ˆë‹¤.
            running_loss += loss.item()  # lossì„ ëˆ„ì í•©ë‹ˆë‹¤.
            _, predicted = output.max(1) # ê°€ì¥ í° ê°’ì˜ ì¸ë±ìŠ¤ë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
            correct += (predicted == y).sum().item() # ì •ë‹µì„ ì¹´ìš´íŠ¸í•©ë‹ˆë‹¤.
            total += y.size(0) # ì „ì²´ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸í•©ë‹ˆë‹¤.
    return running_loss / len(test_loader), correct / total  # í‰ê·  lossê³¼ ì •í™•ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.


def main():
    # ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    model = WithoutNNMLP(4, 100, 3)
    criterion = WithoutNNCrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

    train_loss_list, test_loss_list, test_acc_list = [], [], []
    for epoch in range(50):
        train_loss = train(model, criterion, optimizer, train_loader)
        train_loss_list.append(train_loss)
        test_loss, test_acc = test(model, test_loader, criterion)
        test_loss_list.append(test_loss)
        test_acc_list.append(test_acc)
        print(
            f"Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}"
        )

    plt.plot(train_loss_list, label="train loss")
    plt.plot(test_loss_list, label="test loss")
    plt.legend()
    plt.show()

    plt.plot(test_acc_list, label="test acc")
    plt.legend()
    plt.show()

    return train_loss_list, test_loss_list, test_acc_list
```


    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    Cell In[17], line 29
         25         running_loss += loss.item()  # lossì„ ëˆ„ì í•©ë‹ˆë‹¤.
         26     return running_loss / len(train_loader)  # í‰ê·  lossì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ---> 29 def test(model, test_loader, criterion) -> tuple[float, float]:
         31     running_loss = 0
         32     correct = 0
    

    TypeError: 'type' object is not subscriptable


#### 3.1.1 Train MLP

ìœ„ì—ì„œ ì™„ì„±í•œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ MLP ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

**â“ë¬¸ì œ**
- ìœ„ì—ì„œ ì •ì˜í•œ `train`í•¨ìˆ˜ì™€ `test`í•¨ìˆ˜ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
- í•™ìŠµì´ ì§„í–‰ë¨ì— ë”°ë¼ lossê°€ ê°ì†Œí•˜ê³  ì •í™•ë„ê°€ ì¦ê°€í•˜ëŠ”ì§€ ì§ì ‘ í™•ì¸í•©ë‹ˆë‹¤.


```python
train_loss_list, test_loss_list, test_acc_list = main()
```

    Epoch: 1, Train Loss: 7.7443, Test Loss: 0.8186, Test Acc: 0.7333
    Epoch: 2, Train Loss: 1.8308, Test Loss: 12.8408, Test Acc: 0.3667
    Epoch: 3, Train Loss: 2.9879, Test Loss: 0.8166, Test Acc: 0.8000
    Epoch: 4, Train Loss: 2.0300, Test Loss: 0.0458, Test Acc: 1.0000
    Epoch: 5, Train Loss: 0.1185, Test Loss: 0.4354, Test Acc: 0.9000
    Epoch: 6, Train Loss: 0.1603, Test Loss: 0.1494, Test Acc: 0.9333
    Epoch: 7, Train Loss: 0.6769, Test Loss: 0.7359, Test Acc: 0.7333
    Epoch: 8, Train Loss: 1.1993, Test Loss: 7.9506, Test Acc: 0.6000
    Epoch: 9, Train Loss: 1.7684, Test Loss: 0.5903, Test Acc: 0.7667
    Epoch: 10, Train Loss: 0.4223, Test Loss: 2.8469, Test Acc: 0.6667
    Epoch: 11, Train Loss: 1.2770, Test Loss: 0.7735, Test Acc: 0.8333
    Epoch: 12, Train Loss: 0.1772, Test Loss: 0.0660, Test Acc: 0.9333
    Epoch: 13, Train Loss: 2.6125, Test Loss: 0.1333, Test Acc: 0.9333
    Epoch: 14, Train Loss: 0.8701, Test Loss: 0.0490, Test Acc: 0.9667
    Epoch: 15, Train Loss: 0.4213, Test Loss: 0.1061, Test Acc: 0.9333
    Epoch: 16, Train Loss: 0.1808, Test Loss: 4.2070, Test Acc: 0.6333
    Epoch: 17, Train Loss: 0.5110, Test Loss: 0.0194, Test Acc: 1.0000
    Epoch: 18, Train Loss: 0.0991, Test Loss: 0.0319, Test Acc: 1.0000
    Epoch: 19, Train Loss: 2.7935, Test Loss: 0.0776, Test Acc: 0.9667
    Epoch: 20, Train Loss: 0.9323, Test Loss: 2.5909, Test Acc: 0.7000
    Epoch: 21, Train Loss: 1.8595, Test Loss: 0.4862, Test Acc: 0.9000
    Epoch: 22, Train Loss: 0.1368, Test Loss: 0.0159, Test Acc: 1.0000
    Epoch: 23, Train Loss: 0.2485, Test Loss: 0.1073, Test Acc: 0.9667
    Epoch: 24, Train Loss: 0.1786, Test Loss: 0.8706, Test Acc: 0.8333
    Epoch: 25, Train Loss: 0.9075, Test Loss: 0.0172, Test Acc: 1.0000
    Epoch: 26, Train Loss: 0.2444, Test Loss: 0.0075, Test Acc: 1.0000
    Epoch: 27, Train Loss: 1.2086, Test Loss: 0.0651, Test Acc: 0.9667
    Epoch: 28, Train Loss: 0.2182, Test Loss: 0.0137, Test Acc: 1.0000
    Epoch: 29, Train Loss: 0.2211, Test Loss: 0.0118, Test Acc: 1.0000
    Epoch: 30, Train Loss: 0.0901, Test Loss: 0.0065, Test Acc: 1.0000
    Epoch: 31, Train Loss: 0.6299, Test Loss: 0.0211, Test Acc: 1.0000
    Epoch: 32, Train Loss: 1.2276, Test Loss: 0.1814, Test Acc: 0.9333
    Epoch: 33, Train Loss: 0.5923, Test Loss: 0.7163, Test Acc: 0.8333
    Epoch: 34, Train Loss: 0.1458, Test Loss: 0.0127, Test Acc: 1.0000
    Epoch: 35, Train Loss: 0.1088, Test Loss: 0.0813, Test Acc: 0.9667
    Epoch: 36, Train Loss: 0.3063, Test Loss: 0.0358, Test Acc: 0.9667
    Epoch: 37, Train Loss: 0.1547, Test Loss: 0.2666, Test Acc: 0.9333
    Epoch: 38, Train Loss: 0.0561, Test Loss: 0.0059, Test Acc: 1.0000
    Epoch: 39, Train Loss: 0.0548, Test Loss: 0.0045, Test Acc: 1.0000
    Epoch: 40, Train Loss: 0.3058, Test Loss: 0.0044, Test Acc: 1.0000
    Epoch: 41, Train Loss: 0.1274, Test Loss: 0.0876, Test Acc: 0.9667
    Epoch: 42, Train Loss: 0.0751, Test Loss: 0.0212, Test Acc: 1.0000
    Epoch: 43, Train Loss: 0.0511, Test Loss: 0.0199, Test Acc: 1.0000
    Epoch: 44, Train Loss: 0.2610, Test Loss: 0.0873, Test Acc: 0.9667
    Epoch: 45, Train Loss: 0.2418, Test Loss: 2.7591, Test Acc: 0.7000
    Epoch: 46, Train Loss: 0.9760, Test Loss: 1.0687, Test Acc: 0.8000
    Epoch: 47, Train Loss: 1.6969, Test Loss: 0.0346, Test Acc: 0.9667
    Epoch: 48, Train Loss: 0.3203, Test Loss: 0.0099, Test Acc: 1.0000
    Epoch: 49, Train Loss: 0.0806, Test Loss: 0.3101, Test Acc: 0.9333
    Epoch: 50, Train Loss: 0.2158, Test Loss: 0.0083, Test Acc: 1.0000
    


    
![png](output_28_1.png)
    



    
![png](output_28_2.png)
    


### 4. ëª¨ë¸ êµ¬í˜„(With `torch.nn`)




#### 4.1 MLP components ì„ ì–¸

>ì‚¬ìš©í•  í•¨ìˆ˜ë“¤ì„ ê°ê° í™•ì¸í•©ë‹ˆë‹¤.

**â“ë¬¸ì œ**
- ìœ„ì—ì„œ ì •ì˜í•œ ëª¨ë¸ì„ `torch.nn`ì„ ì´ìš©í•˜ì—¬ êµ¬í˜„í•©ë‹ˆë‹¤.

**ğŸ’¡Hints**
- `torch.nn.Linear`: Linear layerë¥¼ ì •ì˜í•©ë‹ˆë‹¤. [link](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)
- `torch.nn.ReLU`: ReLU layerë¥¼ ì •ì˜í•©ë‹ˆë‹¤. [link](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)
- `torch.nn.CrossEntropyLoss`: Cross Entropy Lossë¥¼ ì •ì˜í•©ë‹ˆë‹¤. [link](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)

#### 4.1.1 Linear layer ì„ ì–¸

**â“ë¬¸ì œ**
- input dataì˜ ***input sizeëŠ” 1***, output dataì˜ ***output sizeëŠ” 3***ì¸ Linear layerë¥¼ ì •ì˜í•©ë‹ˆë‹¤.


```python
import torch.nn as nn

nn_linear = nn.Linear(1, 3)
```


```python
x = torch.FloatTensor([[1.0]])

print("ë³€ê²½ ì „ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ í™•ì¸")
print(nn_linear.weight)
print(nn_linear.bias)

nn_linear.weight = nn.Parameter(torch.FloatTensor([[1.0], [2.0], [3.0]]))
nn_linear.bias = nn.Parameter(torch.FloatTensor([1.0, 1.0, 1.0]))
print("ë³€ê²½ëœ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ í™•ì¸")
print(nn_linear.weight)
print(nn_linear.bias)

out = nn_linear(x)
print("ë³€ê²½ëœ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ í†µê³¼í•œ ì¶œë ¥ í™•ì¸")
print(out)

assert torch.allclose(out, torch.FloatTensor([[2.0, 3.0, 4.0]]))
print("âœ…í…ŒìŠ¤íŠ¸ í†µê³¼!ğŸ¥³")
```

    ë³€ê²½ ì „ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ í™•ì¸
    Parameter containing:
    tensor([[ 0.9436],
            [-0.6645],
            [ 0.8021]], requires_grad=True)
    Parameter containing:
    tensor([-0.0472, -0.6961, -0.3731], requires_grad=True)
    ë³€ê²½ëœ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ í™•ì¸
    Parameter containing:
    tensor([[1.],
            [2.],
            [3.]], requires_grad=True)
    Parameter containing:
    tensor([1., 1., 1.], requires_grad=True)
    ë³€ê²½ëœ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ í†µê³¼í•œ ì¶œë ¥ í™•ì¸
    tensor([[2., 3., 4.]], grad_fn=<AddmmBackward0>)
    âœ…í…ŒìŠ¤íŠ¸ í†µê³¼!ğŸ¥³
    

#### 4.2 ReLU ì„ ì–¸

**â“ë¬¸ì œ**
- ReLU activation functionì„ ì •ì˜í•©ë‹ˆë‹¤.


```python
nn_relu = nn.ReLU()
```

#### 4.2.2 ReLU debug


```python
x = torch.linspace(-0.5, 0.5, 11)

print("ReLU ì ìš© ì „")
print(x)
out = nn_relu(x)
print("ReLU ì ìš© í›„")
print(out)

plt.plot(x, x)
plt.plot(x, out)
plt.show()

assert torch.allclose(
    out, torch.FloatTensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5])
)
print("âœ…í…ŒìŠ¤íŠ¸ í†µê³¼!ğŸ¥³")
```

    ReLU ì ìš© ì „
    tensor([-5.0000e-01, -4.0000e-01, -3.0000e-01, -2.0000e-01, -1.0000e-01,
            -7.4506e-09,  1.0000e-01,  2.0000e-01,  3.0000e-01,  4.0000e-01,
             5.0000e-01])
    ReLU ì ìš© í›„
    tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.2000, 0.3000,
            0.4000, 0.5000])
    


    
![png](output_37_1.png)
    


    âœ…í…ŒìŠ¤íŠ¸ í†µê³¼!ğŸ¥³
    

#### 4.3 Combine components

**â“ë¬¸ì œ**
1. `__init__` ë©”ì†Œë“œì—ì„œ ë¶€ëª¨ í´ë˜ìŠ¤ ìƒì„±ìë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
2. `__init__` ë©”ì†Œë“œì—ì„œ `self.linear1`, `self.relu1`, `self.linear2`, `self.relu2`ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
3. `forward` ë©”ì†Œë“œì—ì„œ MLP ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.


ìœ„ì—ì„œ ì •ì˜í•œ í•¨ìˆ˜ë“¤ì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.


```python
import torch.nn as nn


class MLP(nn.Module):
    def __init__(self, in_features, hidden_features, out_features):
        super(MLP, self).__init__()
        self.linear1 = nn.Linear(in_features, hidden_features)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(hidden_features, out_features)

    def forward(self, x):
        x = self.linear1(x)
        x = self.relu(x)
        x = self.linear2(x)
        return x
```

### 5. ëª¨ë¸ í•™ìŠµ

**â“ë¬¸ì œ**
1. `train_nn` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í•™ìŠµ lossë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
2. `test_nn` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ í‰ê°€í•˜ê³  í‰ê°€ lossì™€ accuracyë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.



```python
# ì‹ ê²½ë§ì„ í•™ìŠµí•˜ëŠ” í•¨ìˆ˜
def train_nn(model, criterion, optimizer, train_loader) -> float:
    model.train()  # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •
    running_loss = 0
    for X, y in train_loader:  # í•™ìŠµ ë°ì´í„°ë¥¼ ë°˜ë³µ
        optimizer.zero_grad()  # ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        output = model(X)  # ëª¨ë¸ì˜ ì¶œë ¥ì„ ê³„ì‚°
        loss = criterion(output, y)  # ì†ì‹¤ì„ ê³„ì‚°
        loss.backward()  # ì—­ì „íŒŒë¥¼ ìˆ˜í–‰
        optimizer.step()  # ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸
        running_loss += loss.item()  # ì†ì‹¤ì„ ëˆ„ì 
    return running_loss / len(train_loader)  # í‰ê·  ì†ì‹¤ì„ ë°˜í™˜


# ì‹ ê²½ë§ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” í•¨ìˆ˜
def test_nn(model, test_loader, criterion) -> tuple[float, float]:
    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    running_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ ë¹„í™œì„±í™”
        for X, y in test_loader:  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë°˜ë³µ
            output = model(X)  # ëª¨ë¸ì˜ ì¶œë ¥ì„ ê³„ì‚°
            loss = criterion(output, y)  # ì†ì‹¤ì„ ê³„ì‚°
            running_loss += loss.item()  # ì†ì‹¤ì„ ëˆ„ì 
            _, predicted = output.max(1)  # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡
            correct += (predicted == y).sum().item()  # ì •í™•í•œ ì˜ˆì¸¡ì˜ ìˆ˜ë¥¼ ëˆ„ì 
            total += y.size(0)  # ì „ì²´ ë ˆì´ë¸”ì˜ ìˆ˜ë¥¼ ëˆ„ì 
    return running_loss / len(test_loader), correct / total  # í‰ê·  ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ë°˜í™˜


# ì‹ ê²½ë§ì„ í•™ìŠµí•˜ê³  í…ŒìŠ¤íŠ¸í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
def main_nn():
    model = MLP(4, 100, 3)  # ëª¨ë¸ì„ ìƒì„±
    criterion = nn.CrossEntropyLoss()  # ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì •ì˜
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì •ì˜

    train_loss_list, test_loss_list, test_acc_list = (
        [],
        [],
        [],
    )  # ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    for epoch in range(50):  # 50 ì—í¬í¬ ë™ì•ˆ í•™ìŠµ
        train_loss = train_nn(
            model, criterion, optimizer, train_loader
        )  # í•™ìŠµ ì†ì‹¤ì„ ê³„ì‚°
        train_loss_list.append(train_loss)  # í•™ìŠµ ì†ì‹¤ì„ ì €ì¥
        test_loss, test_acc = test_nn(
            model, test_loader, criterion
        )  # í…ŒìŠ¤íŠ¸ ì†ì‹¤ê³¼ ì •í™•ë„ë¥¼ ê³„ì‚°
        test_loss_list.append(test_loss)  # í…ŒìŠ¤íŠ¸ ì†ì‹¤ì„ ì €ì¥
        test_acc_list.append(test_acc)  # í…ŒìŠ¤íŠ¸ ì •í™•ë„ë¥¼ ì €ì¥
        print(
            f"Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}"
        )  # ì—í¬í¬, í•™ìŠµ ì†ì‹¤, í…ŒìŠ¤íŠ¸ ì†ì‹¤, í…ŒìŠ¤íŠ¸ ì •í™•ë„ë¥¼ ì¶œë ¥

    # í•™ìŠµ ì†ì‹¤ê³¼ í…ŒìŠ¤íŠ¸ ì†ì‹¤ì„ ê·¸ë˜í”„ë¡œ ê·¸ë¦¼
    plt.plot(train_loss_list, label="train loss")
    plt.plot(test_loss_list, label="test loss")
    plt.legend()
    plt.show()

    # í…ŒìŠ¤íŠ¸ ì •í™•ë„ë¥¼ ê·¸ë˜í”„ë¡œ ê·¸ë¦¼
    plt.plot(test_acc_list, label="test acc")
    plt.legend()
    plt.show()

    return train_loss_list, test_loss_list, test_acc_list
```


```python
nn_train_loss_list, nn_test_loss_list, nn_test_acc_list = main_nn()
```

    Epoch: 1, Train Loss: 0.9669, Test Loss: 1.0201, Test Acc: 0.4000
    Epoch: 2, Train Loss: 0.9437, Test Loss: 0.7764, Test Acc: 0.9667
    Epoch: 3, Train Loss: 0.7770, Test Loss: 0.6805, Test Acc: 0.6667
    Epoch: 4, Train Loss: 0.7012, Test Loss: 0.6541, Test Acc: 0.6667
    Epoch: 5, Train Loss: 0.7016, Test Loss: 0.8287, Test Acc: 0.6667
    Epoch: 6, Train Loss: 0.6690, Test Loss: 0.5680, Test Acc: 0.7000
    Epoch: 7, Train Loss: 0.6268, Test Loss: 0.5231, Test Acc: 0.8000
    Epoch: 8, Train Loss: 0.5330, Test Loss: 0.5004, Test Acc: 0.9000
    Epoch: 9, Train Loss: 0.5848, Test Loss: 0.4820, Test Acc: 0.9000
    Epoch: 10, Train Loss: 0.5170, Test Loss: 0.6367, Test Acc: 0.6667
    Epoch: 11, Train Loss: 0.5237, Test Loss: 0.4462, Test Acc: 1.0000
    Epoch: 12, Train Loss: 0.5047, Test Loss: 0.4367, Test Acc: 0.8000
    Epoch: 13, Train Loss: 0.4651, Test Loss: 0.4621, Test Acc: 0.6667
    Epoch: 14, Train Loss: 0.4753, Test Loss: 0.5252, Test Acc: 0.6667
    Epoch: 15, Train Loss: 0.5096, Test Loss: 0.5673, Test Acc: 0.6667
    Epoch: 16, Train Loss: 0.5164, Test Loss: 0.5405, Test Acc: 0.6667
    Epoch: 17, Train Loss: 0.4679, Test Loss: 0.3866, Test Acc: 0.9333
    Epoch: 18, Train Loss: 0.4582, Test Loss: 0.4197, Test Acc: 0.7667
    Epoch: 19, Train Loss: 0.4502, Test Loss: 0.4077, Test Acc: 0.7000
    Epoch: 20, Train Loss: 0.4396, Test Loss: 0.3772, Test Acc: 0.9000
    Epoch: 21, Train Loss: 0.4209, Test Loss: 0.4799, Test Acc: 0.6667
    Epoch: 22, Train Loss: 0.3858, Test Loss: 0.4103, Test Acc: 0.7000
    Epoch: 23, Train Loss: 0.3953, Test Loss: 0.3308, Test Acc: 1.0000
    Epoch: 24, Train Loss: 0.3743, Test Loss: 0.5365, Test Acc: 0.6667
    Epoch: 25, Train Loss: 0.4367, Test Loss: 0.3364, Test Acc: 0.9000
    Epoch: 26, Train Loss: 0.4245, Test Loss: 0.3191, Test Acc: 0.9333
    Epoch: 27, Train Loss: 0.3927, Test Loss: 0.3390, Test Acc: 0.8000
    Epoch: 28, Train Loss: 0.3674, Test Loss: 0.3121, Test Acc: 0.9667
    Epoch: 29, Train Loss: 0.3354, Test Loss: 0.3671, Test Acc: 0.7000
    Epoch: 30, Train Loss: 0.3444, Test Loss: 0.2879, Test Acc: 1.0000
    Epoch: 31, Train Loss: 0.3923, Test Loss: 0.2827, Test Acc: 1.0000
    Epoch: 32, Train Loss: 0.3144, Test Loss: 0.2858, Test Acc: 0.9333
    Epoch: 33, Train Loss: 0.3225, Test Loss: 0.2873, Test Acc: 0.9667
    Epoch: 34, Train Loss: 0.3560, Test Loss: 0.2710, Test Acc: 1.0000
    Epoch: 35, Train Loss: 0.2967, Test Loss: 0.2868, Test Acc: 0.9000
    Epoch: 36, Train Loss: 0.3110, Test Loss: 0.2591, Test Acc: 1.0000
    Epoch: 37, Train Loss: 0.3239, Test Loss: 0.2876, Test Acc: 0.9000
    Epoch: 38, Train Loss: 0.3140, Test Loss: 0.3580, Test Acc: 0.7000
    Epoch: 39, Train Loss: 0.3007, Test Loss: 0.2748, Test Acc: 0.9000
    Epoch: 40, Train Loss: 0.2903, Test Loss: 0.2441, Test Acc: 1.0000
    Epoch: 41, Train Loss: 0.2871, Test Loss: 0.2424, Test Acc: 1.0000
    Epoch: 42, Train Loss: 0.3214, Test Loss: 0.3250, Test Acc: 0.8000
    Epoch: 43, Train Loss: 0.3020, Test Loss: 0.2612, Test Acc: 0.9333
    Epoch: 44, Train Loss: 0.2921, Test Loss: 0.2286, Test Acc: 1.0000
    Epoch: 45, Train Loss: 0.2702, Test Loss: 0.2288, Test Acc: 1.0000
    Epoch: 46, Train Loss: 0.2930, Test Loss: 0.2512, Test Acc: 0.9000
    Epoch: 47, Train Loss: 0.3034, Test Loss: 0.2944, Test Acc: 0.8000
    Epoch: 48, Train Loss: 0.2808, Test Loss: 0.2155, Test Acc: 1.0000
    Epoch: 49, Train Loss: 0.2585, Test Loss: 0.2627, Test Acc: 0.9000
    Epoch: 50, Train Loss: 0.2494, Test Loss: 0.2112, Test Acc: 1.0000
    


    
![png](output_42_1.png)
    



    
![png](output_42_2.png)
    


### 6. ë¹„êµ

`torch.nn`ì„ ì´ìš©í•˜ì—¬ êµ¬í˜„í•œ ëª¨ë¸ê³¼ `torch.nn`ì„ ì´ìš©í•˜ì§€ ì•Šê³  êµ¬í˜„í•œ ëª¨ë¸ì„ ë¹„êµ í•´ë´…ì‹œë‹¤.


```python
plt.plot(train_loss_list, label="train loss")
plt.plot(test_loss_list, label="test loss")
plt.plot(nn_train_loss_list, label="nn train loss")
plt.plot(nn_test_loss_list, label="nn test loss")
plt.legend()
plt.show()
```


    
![png](output_44_0.png)
    


í•™ìŠµ ì´ˆê¸°ì— ë‘ ëª¨ë¸ì„ ë¹„êµí•˜ë©´ loss ê°’ì´ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì— ëŒ€í•œ ì´ìœ ë¥¼ ìƒê°í•´ë³´ê³  ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì„ ì°¾ì•„ë³´ì„¸ìš”.

#### [Hint]

- [`torch.nn.Linear`](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear)ì˜ ì†ŒìŠ¤ì½”ë“œë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.


```python
import math


# ì„ í˜• ë ˆì´ì–´ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ ë ˆì´ì–´ëŠ” ì…ë ¥ íŠ¹ì„±ê³¼ ì¶œë ¥ íŠ¹ì„±ì˜ ìˆ˜ë¥¼ ë°›ì•„ì„œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
class WithoutNNLinear:
    def __init__(self, in_features, out_features):
        self.in_features = in_features
        self.out_features = out_features
        self.weight = torch.randn(
            self.out_features, self.in_features
        )  # ê°€ì¤‘ì¹˜ëŠ” ëœë¤í•˜ê²Œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        self.bias = torch.randn(self.out_features)  # í¸í–¥ë„ ëœë¤í•˜ê²Œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        self.reset_parameters()

    def reset_parameters(self):
        bound = 1 / math.sqrt(self.in_features)
        self.weight = torch.FloatTensor(self.out_features, self.in_features).uniform_(
            -bound, bound
        )
        self.bias = torch.FloatTensor(self.out_features).uniform_(-bound, bound)

    # ì´ ë©”ì†Œë“œëŠ” ì…ë ¥ xë¥¼ ë°›ì•„ì„œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì´ìš©í•˜ì—¬ ì¶œë ¥ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    def __call__(self, x):
        return (
            x @ self.weight.t() + self.bias
        )  # xì™€ ê°€ì¤‘ì¹˜ì˜ í–‰ë ¬ ê³±ì…ˆì„ ìˆ˜í–‰í•˜ê³  í¸í–¥ì„ ë”í•©ë‹ˆë‹¤.

    # ì´ ë©”ì†Œë“œëŠ” ëª¨ë¸ì„ íŠ¹ì • ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.
    def set_divce(self, device):
        self.weight = self.weight.to(device)
        self.bias = self.bias.to(device)

    # ì´ ë©”ì†Œë“œëŠ” ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    def parameters(self):
        return [self.weight, self.bias]
```

#### ì¬í•™ìŠµ
> init methodë¥¼ ì‚¬ìš©í•˜ëŠ” MLP ì¬í•™ìŠµ ì§„í–‰ í›„ í•™ìŠµ logë¥¼ ë¹„êµí•©ë‹ˆë‹¤.


```python
train_loss_list, test_loss_list, test_acc_list = main()
```

    Epoch: 1, Train Loss: 1.0386, Test Loss: 1.0078, Test Acc: 0.3333
    Epoch: 2, Train Loss: 0.8862, Test Loss: 0.7779, Test Acc: 0.6667
    Epoch: 3, Train Loss: 0.8284, Test Loss: 0.7536, Test Acc: 0.6667
    Epoch: 4, Train Loss: 0.6773, Test Loss: 0.8460, Test Acc: 0.6667
    Epoch: 5, Train Loss: 0.6627, Test Loss: 0.5879, Test Acc: 0.6667
    Epoch: 6, Train Loss: 0.6215, Test Loss: 0.6818, Test Acc: 0.6667
    Epoch: 7, Train Loss: 0.6331, Test Loss: 0.5510, Test Acc: 0.6667
    Epoch: 8, Train Loss: 0.5695, Test Loss: 0.5145, Test Acc: 0.6667
    Epoch: 9, Train Loss: 0.5243, Test Loss: 0.4860, Test Acc: 0.9000
    Epoch: 10, Train Loss: 0.5667, Test Loss: 0.4953, Test Acc: 0.6667
    Epoch: 11, Train Loss: 0.4593, Test Loss: 0.6433, Test Acc: 0.6667
    Epoch: 12, Train Loss: 0.4804, Test Loss: 0.4971, Test Acc: 0.6667
    Epoch: 13, Train Loss: 0.4769, Test Loss: 0.5241, Test Acc: 0.6667
    Epoch: 14, Train Loss: 0.4803, Test Loss: 0.4576, Test Acc: 0.7000
    Epoch: 15, Train Loss: 0.4715, Test Loss: 0.5490, Test Acc: 0.6667
    Epoch: 16, Train Loss: 0.4435, Test Loss: 0.3977, Test Acc: 1.0000
    Epoch: 17, Train Loss: 0.4390, Test Loss: 0.3878, Test Acc: 1.0000
    Epoch: 18, Train Loss: 0.4194, Test Loss: 0.3909, Test Acc: 0.8000
    Epoch: 19, Train Loss: 0.4112, Test Loss: 0.4301, Test Acc: 0.6667
    Epoch: 20, Train Loss: 0.4107, Test Loss: 0.3787, Test Acc: 0.9000
    Epoch: 21, Train Loss: 0.3883, Test Loss: 0.4174, Test Acc: 0.6667
    Epoch: 22, Train Loss: 0.4052, Test Loss: 0.3486, Test Acc: 0.9333
    Epoch: 23, Train Loss: 0.3698, Test Loss: 0.3978, Test Acc: 0.7667
    Epoch: 24, Train Loss: 0.4690, Test Loss: 0.4336, Test Acc: 0.7000
    Epoch: 25, Train Loss: 0.4124, Test Loss: 0.3371, Test Acc: 0.9000
    Epoch: 26, Train Loss: 0.3650, Test Loss: 0.3314, Test Acc: 0.9000
    Epoch: 27, Train Loss: 0.3546, Test Loss: 0.3343, Test Acc: 0.9000
    Epoch: 28, Train Loss: 0.4140, Test Loss: 0.3063, Test Acc: 1.0000
    Epoch: 29, Train Loss: 0.3424, Test Loss: 0.3148, Test Acc: 0.9000
    Epoch: 30, Train Loss: 0.3567, Test Loss: 0.3228, Test Acc: 0.9000
    Epoch: 31, Train Loss: 0.3479, Test Loss: 0.3084, Test Acc: 0.9000
    Epoch: 32, Train Loss: 0.3672, Test Loss: 0.4014, Test Acc: 0.7000
    Epoch: 33, Train Loss: 0.3789, Test Loss: 0.3145, Test Acc: 0.9000
    Epoch: 34, Train Loss: 0.3246, Test Loss: 0.2720, Test Acc: 1.0000
    Epoch: 35, Train Loss: 0.3143, Test Loss: 0.2900, Test Acc: 0.9000
    Epoch: 36, Train Loss: 0.3002, Test Loss: 0.2792, Test Acc: 0.9333
    Epoch: 37, Train Loss: 0.3024, Test Loss: 0.2566, Test Acc: 1.0000
    Epoch: 38, Train Loss: 0.3040, Test Loss: 0.3557, Test Acc: 0.7667
    Epoch: 39, Train Loss: 0.3185, Test Loss: 0.2687, Test Acc: 0.9000
    Epoch: 40, Train Loss: 0.2800, Test Loss: 0.2743, Test Acc: 0.9000
    Epoch: 41, Train Loss: 0.3067, Test Loss: 0.2406, Test Acc: 1.0000
    Epoch: 42, Train Loss: 0.2987, Test Loss: 0.2602, Test Acc: 0.9333
    Epoch: 43, Train Loss: 0.2874, Test Loss: 0.2565, Test Acc: 0.9000
    Epoch: 44, Train Loss: 0.2747, Test Loss: 0.2679, Test Acc: 0.9000
    Epoch: 45, Train Loss: 0.2740, Test Loss: 0.2603, Test Acc: 0.9000
    Epoch: 46, Train Loss: 0.2780, Test Loss: 0.2286, Test Acc: 0.9333
    Epoch: 47, Train Loss: 0.2708, Test Loss: 0.2228, Test Acc: 0.9667
    Epoch: 48, Train Loss: 0.2731, Test Loss: 0.2146, Test Acc: 1.0000
    Epoch: 49, Train Loss: 0.2625, Test Loss: 0.2167, Test Acc: 0.9667
    Epoch: 50, Train Loss: 0.2595, Test Loss: 0.2177, Test Acc: 1.0000
    


    
![png](output_50_1.png)
    



    
![png](output_50_2.png)
    



```python
plt.plot(train_loss_list, label="train loss")
plt.plot(test_loss_list, label="test loss")
plt.plot(nn_train_loss_list, label="nn train loss")
plt.plot(nn_test_loss_list, label="nn test loss")
plt.legend()
plt.show()
```


    
![png](output_51_0.png)
    


## ì½˜í…ì¸  ë¼ì´ì„ ìŠ¤

<hr style="height:5px;border:none;color:#5F71F7;background-color:#5F71F7">

<font color='red'><b>WARNING</font> : ë³¸ êµìœ¡ ì½˜í…ì¸ ì˜ ì§€ì‹ì¬ì‚°ê¶Œì€ ì¬ë‹¨ë²•ì¸ ë„¤ì´ë²„ì»¤ë„¥íŠ¸ì— ê·€ì†ë©ë‹ˆë‹¤. ë³¸ ì½˜í…ì¸ ë¥¼ ì–´ë– í•œ ê²½ë¡œë¡œë“  ì™¸ë¶€ë¡œ ìœ ì¶œ ë° ìˆ˜ì •í•˜ëŠ” í–‰ìœ„ë¥¼ ì—„ê²©íˆ ê¸ˆí•©ë‹ˆë‹¤. ë‹¤ë§Œ, ë¹„ì˜ë¦¬ì  êµìœ¡ ë° ì—°êµ¬í™œë™ì— í•œì •ë˜ì–´ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë‚˜ ì¬ë‹¨ì˜ í—ˆë½ì„ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„ë°˜í•˜ëŠ” ê²½ìš°, ê´€ë ¨ ë²•ë¥ ì— ë”°ë¼ ì±…ì„ì„ ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. </b>


```python

```
